---
title: Migrate from Postgres to ClickHouse
sidebar: Docs
showTitle: true
---

> This documentation is in **Alpha**. Please provide feedback via the [PostHog Communty Slack workspace](/slack) or [via a GitHub issue](https://github.com/PostHog/posthog.com/issues).

**PostHog backed by Postgres is now deprecated**. We will continue to provide support to Postgres backed installs but we strongly encourage you to migrate to PostHog backed by ClickHouse for improved performance, to receive new features, and continued support in the future.

TODOs:
- Confirm what will and will not be migrated

## What will be migrated

The following will be migrated:

- Insights
- Dashboards
- Sessions
- Annotations
- Feature flags
- Projects

## What will not be migrated

- Cohorts

## Before you begin

TODOs:
- Is there a minimum version for PostHog + Postgres and a minimum version for PostHog + ClickHouse? For example, does the Replicator plugin have a minimum PostHog version to work within?

In order to migrate you must have the following in place:

1. The PostHog + Postgres installation
2. A new PostHog + ClickHouse installation

## Export

TODOs:
- Are these the correct tables to dump?
- Should we be using `--data-only` to only dump the data and not the schema creation?

The first step in the migration process is to create a Postgres database dump. With a PostHog + Postgres instance up and running, run the `pg_dump` command with the following format:

```
$ pg_dump -d [database] -h [host] -U [user] -T posthog_person -T posthog_persondistinctid -T posthog_event -T posthog_pluginlogentry -T posthog_sessionrecordingevent --data-only -f [output-file]
```

For example:

```
$ pg_dump -d posthog -h localhost -U posthog -T posthog_person -T posthog_persondistinctid -T posthog_event -T posthog_pluginlogentry -T posthog_sessionrecordingevent --data-only -f posthog-migration.sql
```

The example command above creates a `posthog-migration.sql` that contains SQL statements to insert the data only, excluding event data, from the existing Postgres database.

## Import

The next step is to get the data into the new PostHog + ClickHouse installation. This installation still utilizes Postgres, but only for non-event data. Event data is now stored in ClickHouse.

The import process has two steps:

1. Run the exported SQL against the Postgres database for the new installation
2. Use the PostHog replicator plugin to migrate event data from the old PostHog instance to the new one

### Postgres import of non-event data

Import the old Postgres data into the new PostHog Postgres database. The command should take the following format:

```
psql --dbname="postgres://[user]:[password]@[host]:[port]/[database]" < [input-file]
```

For example:

```
psql --dbname="postgres://postgres:postgres@localhost:5432/posthog" < posthog-migration.sql
```

### ClickHouse import of event data

TODOs:
- Does this need to be done for each project or does it handle multiple projects?
- What should the user do after clicking **Save**?

With the non-event data now migrated to the new PostHog installation, we can now migrate the event data using the [Replicator plugin](https://posthog.com/plugins/replicator).

Within the older deployment of PostHog, go to the PostHog dashboard, then to the **Plugins** section, and select the **Repository** tab. Find the **Replicator** plugin listed on that page and click the **Install* button. Once installed you will be prompted to provide configuration for the plugin.

- **Host**: enter the hostname for your new PostHog instance
- **Project API Key**: go to `{host}/project/settings` and use the value under **Project API Key**
- **Replication**: keep the value of `1`

Then click **Save**.

### Verify the migration

TODOs:
- Provide some sort validation that the data has been migrated
- Conclusion and where to go next